{"authors": ["Harry Davies", "Bethan Mckernan", "Dan Sabbagh"], "date_download": "2024-02-05 15:32:31", "date_modify": "None", "date_publish": "2023-12-01 10:03:58", "description": "Concerns over data-driven \u2018factory\u2019 that significantly increases the number of targets for strikes in the Palestinian territory", "filename": "https%3A%2F%2Fwww.theguardian.com%2Fworld%2F2023%2Fdec%2F01%2Fthe-gospel-how-israel-uses-ai-to-select-bombing-targets.json", "image_url": "https://i.guim.co.uk/img/media/a51fd34ab0ab88f5582f2a441140ac7f681a82b6/0_624_5504_3302/master/5504.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=bc643d21beb7d922f29d46255e6c996c", "language": "en", "localpath": null, "maintext": "Israel\u2019s military has made no secret of the intensity of its bombardment of the Gaza Strip. In the early days of the offensive, the head of its air force spoke of relentless, \u201caround the clock\u201d airstrikes. His forces, he said, were only striking military targets, but he added: \u201cWe are not being surgical.\u201d\nThere has, however, been relatively little attention paid to the methods used by the Israel Defense Forces (IDF) to select targets in Gaza, and to the role artificial intelligence has played in their bombing campaign.\nAs Israel resumes its offensive after a seven-day ceasefire, there are mounting concerns about the IDF\u2019s targeting approach in a war against Hamas that, according to the health ministry in Hamas-run Gaza, has so far killed more than 15,000 people in the territory.\nThe IDF has long burnished its reputation for technical prowess and has previously made bold but unverifiable claims about harnessing new technology. After the 11-day war in Gaza in May 2021, officials said Israel had fought its \u201cfirst AI war\u201d using machine learning and advanced computing.\nThe latest Israel-Hamas war has provided an unprecedented opportunity for the IDF to use such tools in a much wider theatre of operations and, in particular, to deploy an AI target-creation platform called \u201cthe Gospel\u201d, which has significantly accelerated a lethal production line of targets that officials have compared to a \u201cfactory\u201d.\nThe Guardian can reveal new details about the Gospel and its central role in Israel\u2019s war in Gaza, using interviews with intelligence sources and little-noticed statements made by the IDF and retired officials.\nThis article also draws on testimonies published by the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call, which have interviewed several current and former sources in Israel\u2019s intelligence community who have knowledge of the Gospel platform.\nTheir comments offer a glimpse inside a secretive, AI-facilitated military intelligence unit that is playing a significant role in Israel\u2019s response to the Hamas massacre in southern Israel on 7 October.\nThe slowly emerging picture of how Israel\u2019s military is harnessing AI comes against a backdrop of growing concerns about the risks posed to civilians as advanced militaries around the world expand the use of complex and opaque automated systems on the battlefield.\n\u201cOther states are going to be watching and learning,\u201d said a former White House security official familiar with the US military\u2019s use of autonomous systems.\nThe Israel-Hamas war, they said, would be an \u201cimportant moment if the IDF is using AI in a significant way to make targeting choices with life-and-death consequences\u201d.\nView image in fullscreen Israeli soldiers during ground operations in the Gaza Strip. Photograph: IDF\nFrom 50 targets a year to 100 a day\nIn early November, the IDF said \u201cmore than 12,000\u201d targets in Gaza had been identified by its target administration division.\nDescribing the unit\u2019s targeting process, an official said: \u201cWe work without compromise in defining who and what the enemy is. The operatives of Hamas are not immune \u2013 no matter where they hide.\u201d\nThe activities of the division, formed in 2019 in the IDF\u2019s intelligence directorate, are classified.\nHowever a short statement on the IDF website claimed it was using an AI-based system called Habsora (the Gospel, in English) in the war against Hamas to \u201cproduce targets at a fast pace\u201d.\nThe IDF said that \u201cthrough the rapid and automatic extraction of intelligence\u201d, the Gospel produced targeting recommendations for its researchers \u201cwith the goal of a complete match between the recommendation of the machine and the identification carried out by a person\u201d.\nMultiple sources familiar with the IDF\u2019s targeting processes confirmed the existence of the Gospel to +972/Local Call, saying it had been used to produce automated recommendations for attacking targets, such as the private homes of individuals suspected of being Hamas or Islamic Jihad operatives.\nIn recent years, the target division has helped the IDF build a database of what sources said was between 30,000 and 40,000 suspected militants. Systems such as the Gospel, they said, had played a critical role in building lists of individuals authorised to be assassinated.\nAviv Kochavi, who served as the head of the IDF until January, has said the target division is \u201cpowered by AI capabilities\u201d and includes hundreds of officers and soldiers.\nIn an interview published before the war, he said it was \u201ca machine that produces vast amounts of data more effectively than any human, and translates it into targets for attack\u201d.\nView image in fullscreen Aviv Kochavi in his role as head of the IDF in 2019. Photograph: Oded Balilty/AP\nAccording to Kochavi, \u201conce this machine was activated\u201d in Israel\u2019s 11-day war with Hamas in May 2021 it generated 100 targets a day. \u201cTo put that into perspective, in the past we would produce 50 targets in Gaza per year. Now, this machine produces 100 targets a single day, with 50% of them being attacked.\u201d\nPrecisely what forms of data are ingested into the Gospel is not known. But experts said AI-based decision support systems for targeting would typically analyse large sets of information from a range of sources, such as drone footage, intercepted communications, surveillance data and information drawn from monitoring the movements and behaviour patterns of individuals and large groups.\nThe target division was created to address a chronic problem for the IDF: in earlier operations in Gaza, the air force repeatedly ran out of targets to strike. Since senior Hamas officials disappeared into tunnels at the start of any new offensive, sources said, systems such as the Gospel allowed the IDF to locate and attack a much larger pool of more junior operatives.\nOne official, who worked on targeting decisions in previous Gaza operations, said the IDF had not previously targeted the homes of junior Hamas members for bombings. They said they believed that had changed for the present conflict, with the houses of suspected Hamas operatives now targeted regardless of rank.\n\u201cThat is a lot of houses,\u201d the official told +972/Local Call. \u201cHamas members who don\u2019t really mean anything live in homes across Gaza. So they mark the home and bomb the house and kill everyone there.\u201d\nTargets given \u2018score\u2019 for likely civilian death toll\nIn the IDF\u2019s brief statement about its target division, a senior official said the unit \u201cproduces precise attacks on infrastructure associated with Hamas while inflicting great damage to the enemy and minimal harm to non-combatants\u201d.\nThe precision of strikes recommended by the \u201cAI target bank\u201d has been emphasised in multiple reports in Israeli media. The Yedioth Ahronoth daily newspaper reported that the unit \u201cmakes sure as far as possible there will be no harm to non-involved civilians\u201d.\nA former senior Israeli military source told the Guardian that operatives use a \u201cvery accurate\u201d measurement of the rate of civilians evacuating a building shortly before a strike. \u201cWe use an algorithm to evaluate how many civilians are remaining. It gives us a green, yellow, red, like a traffic signal.\u201d\nHowever, experts in AI and armed conflict who spoke to the Guardian said they were sceptical of assertions that AI-based systems reduced civilian harm by encouraging more accurate targeting.\nA lawyer who advises governments on AI and compliance with humanitarian law said there was \u201clittle empirical evidence\u201d to support such claims. Others pointed to the visible impact of the bombardment.\n\u201cLook at the physical landscape of Gaza,\u201d said Richard Moyes, a researcher who heads Article 36, a group that campaigns to reduce harm from weapons.\n\u201cWe\u2019re seeing the widespread flattening of an urban area with heavy explosive weapons, so to claim there\u2019s precision and narrowness of force being exerted is not borne out by the facts.\u201d\nView image in fullscreen Satellite images of the northern city of Beit Hanoun in Gaza before (10 October) and after (21 October) damage caused by the war. Photograph: Maxar Technologies/Reuters\nAccording to figures released by the IDF in November, during the first 35 days of the war Israel attacked 15,000 targets in Gaza, a figure that is considerably higher than previous military operations in the densely populated coastal territory. By comparison, in the 2014 war, which lasted 51 days, the IDF struck between 5,000 and 6,000 targets.\nMultiple sources told the Guardian and +972/Local Call that when a strike was authorised on the private homes of individuals identified as Hamas or Islamic Jihad operatives, target researchers knew in advance the number of civilians expected to be killed.\nEach target, they said, had a file containing a collateral damage score that stipulated how many civilians were likely to be killed in a strike.\nOne source who worked until 2021 on planning strikes for the IDF said \u201cthe decision to strike is taken by the on-duty unit commander\u201d, some of whom were \u201cmore trigger happy than others\u201d.\nThe source said there had been occasions when \u201cthere was doubt about a target\u201d and \u201cwe killed what I thought was a disproportionate amount of civilians\u201d.\nAn Israeli military spokesperson said: \u201cIn response to Hamas\u2019 barbaric attacks, the IDF operates to dismantle Hamas military and administrative capabilities. In stark contrast to Hamas\u2019 intentional attacks on Israeli men, women and children, the IDF follows international law and takes feasible precautions to mitigate civilian harm.\u201d\n\u2018Mass assassination factory\u2019\nSources familiar with how AI-based systems have been integrated into the IDF\u2019s operations said such tools had significantly sped up the target creation process.\n\u201cWe prepare the targets automatically and work according to a checklist,\u201d a source who previously worked in the target division told +972/Local Call. \u201cIt really is like a factory. We work quickly and there is no time to delve deep into the target. The view is that we are judged according to how many targets we manage to generate.\u201d\nA separate source told the publication the Gospel had allowed the IDF to run a \u201cmass assassination factory\u201d in which the \u201cemphasis is on quantity and not on quality\u201d. A human eye, they said, \u201cwill go over the targets before each attack, but it need not spend a lot of time on them\u201d.\nFor some experts who research AI and international humanitarian law, an acceleration of this kind raises a number of concerns.\nDr Marta Bo, a researcher at the Stockholm International Peace Research Institute, said that even when \u201chumans are in the loop\u201d there is a risk they develop \u201cautomation bias\u201d and \u201cover-rely on systems which come to have too much influence over complex human decisions\u201d.\nMoyes, of Article 36, said that when relying on tools such as the Gospel, a commander \u201cis handed a list of targets a computer has generated\u201d and they \u201cdon\u2019t necessarily know how the list has been created or have the ability to adequately interrogate and question the targeting recommendations\u201d.\n\u201cThere is a danger,\u201d he added, \u201cthat as humans come to rely on these systems they become cogs in a mechanised process and lose the ability to consider the risk of civilian harm in a meaningful way.\u201d", "source_domain": "www.theguardian.com", "text": null, "title": "\u2018The Gospel\u2019: how Israel uses AI to select bombing targets in Gaza", "title_page": null, "title_rss": null, "url": "https://www.theguardian.com/world/2023/dec/01/the-gospel-how-israel-uses-ai-to-select-bombing-targets"}